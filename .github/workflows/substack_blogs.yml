name: Fetch Substack Blog Posts
on:
  schedule: # Run workflow automatically
    - cron: '0 0 * * *' # Runs once a day at midnight
  workflow_dispatch: # Run workflow manually through the GitHub UI

jobs:
  fetch-substack-posts:
    name: Fetch latest blog posts from Substack
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          ref: master  # Explicitly checkout the master branch

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests beautifulsoup4 python-dateutil fake-useragent

      - name: Run script to fetch Substack posts and update README
        uses: jannekem/run-python-script-action@v1
        with:
          script: |
            import feedparser
            import random
            import time
            import requests
            import os
            from datetime import datetime
            from fake_useragent import UserAgent
            from bs4 import BeautifulSoup

            def fetch_substack_feed(feed_url):
                """Fetch Substack feed with proxy rotation to avoid IP blocks"""
                
                # Try free proxy services (consider using a paid proxy service for production)
                free_proxy_lists = [
                    "https://free-proxy-list.net/",
                    "https://www.sslproxies.org/",
                    "https://www.us-proxy.org/"
                ]
                
                proxies = get_proxies(free_proxy_lists)
                
                # Set up headers with random user agent
                ua = UserAgent()
                headers = {"User-Agent": ua.random}
                
                # First try without proxy
                try:
                    print("Attempting to fetch feed directly...")
                    response = requests.get(feed_url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        print("Direct fetch successful!")
                        return feedparser.parse(response.content)
                except Exception as e:
                    print(f"Direct fetch failed: {e}")
                
                # If direct fetch fails, try with proxies
                if proxies:
                    for proxy in proxies:
                        try:
                            print(f"Trying proxy: {proxy}")
                            proxy_dict = {
                                "http": f"http://{proxy}",
                                "https": f"http://{proxy}"
                            }
                            response = requests.get(feed_url, headers=headers, proxies=proxy_dict, timeout=15)
                            if response.status_code == 200:
                                print(f"Proxy fetch successful with {proxy}")
                                return feedparser.parse(response.content)
                        except Exception as e:
                            print(f"Proxy fetch failed with {proxy}: {e}")
                        
                        # Add delay between requests
                        time.sleep(1)
                
                # Fallback: use a public RSS proxy service
                print("Trying fallback method with RSS proxy...")
                try:
                    # Using a public RSS-to-JSON service as proxy
                    rss_proxy_url = f"https://api.rss2json.com/v1/api.json?rss_url={feed_url}"
                    response = requests.get(rss_proxy_url, headers=headers, timeout=15)
                    if response.status_code == 200:
                        print("Fallback method successful!")
                        json_data = response.json()
                        
                        # Convert the JSON response to format similar to feedparser
                        feed_data = {"entries": []}
                        if "items" in json_data:
                            for item in json_data["items"]:
                                feed_data["entries"].append({
                                    "title": item.get("title", ""),
                                    "link": item.get("link", ""),
                                    "published": item.get("pubDate", ""),
                                    "summary": item.get("description", "")
                                })
                        return feed_data
                    else:
                        print(f"Fallback method failed with status code: {response.status_code}")
                except Exception as e:
                    print(f"Fallback method failed: {e}")
                
                # Additional fallback: Try alternative RSS services that can proxy the request
                print("Trying alternative RSS services...")
                alternative_services = [
                    f"https://feedproxy.google.com/{feed_url}",
                    f"https://feed2json.org/convert?url={feed_url}",
                    f"https://cloud.feedly.com/v3/streams/contents?streamId=feed%2F{feed_url}"
                ]
                
                for service_url in alternative_services:
                    try:
                        print(f"Trying service: {service_url}")
                        response = requests.get(service_url, headers=headers, timeout=15)
                        if response.status_code == 200:
                            print(f"Service fetch successful with {service_url}")
                            try:
                                return feedparser.parse(response.content)
                            except:
                                # If feedparser fails, try to extract data from JSON response
                                try:
                                    json_data = response.json()
                                    feed_data = {"entries": []}
                                    
                                    # Handle different service response formats
                                    if "items" in json_data:
                                        items = json_data["items"]
                                    elif "contents" in json_data:
                                        items = json_data["contents"]
                                    else:
                                        items = []
                                    
                                    for item in items:
                                        entry = {
                                            "title": item.get("title", ""),
                                            "link": item.get("link", "") or item.get("url", "") or item.get("alternate", [{}])[0].get("href", ""),
                                            "published": item.get("published", "") or item.get("pubDate", "") or item.get("updated", ""),
                                            "summary": item.get("description", "") or item.get("summary", "") or item.get("content", "")
                                        }
                                        feed_data["entries"].append(entry)
                                    
                                    return feed_data
                                except:
                                    print("Failed to parse JSON response")
                    except Exception as e:
                        print(f"Service fetch failed with {service_url}: {e}")
                    
                    # Add delay between requests
                    time.sleep(1)
                
                # Last resort: Try to scrape the HTML page directly
                try:
                    print("Trying to scrape the HTML page directly...")
                    substack_url = feed_url.replace("/feed", "")
                    response = requests.get(substack_url, headers=headers, timeout=20)
                    
                    if response.status_code == 200:
                        from bs4 import BeautifulSoup
                        
                        soup = BeautifulSoup(response.text, 'html.parser')
                        entries = []
                        
                        # Find all post entries on the page
                        post_elements = soup.select('.post-preview')
                        
                        for i, post in enumerate(post_elements[:5]):  # Only process up to 5 posts
                            try:
                                title_elem = post.select_one('.post-preview-title')
                                link_elem = post.select_one('a.post-preview-content')
                                date_elem = post.select_one('.post-preview-date')
                                
                                title = title_elem.text.strip() if title_elem else "No title"
                                link = link_elem['href'] if link_elem and 'href' in link_elem.attrs else ""
                                published = date_elem.text.strip() if date_elem else ""
                                
                                # Make sure links are absolute
                                if link and not link.startswith('http'):
                                    link = f"{substack_url.rstrip('/')}{link}"
                                
                                entries.append({
                                    'title': title,
                                    'link': link,
                                    'published': published,
                                    'summary': ''
                                })
                            except Exception as e:
                                print(f"Error parsing post {i}: {e}")
                        
                        if entries:
                            print("Successfully scraped posts from HTML!")
                            return {"entries": entries}
                except Exception as e:
                    print(f"HTML scraping failed: {e}")
                
                # If all methods fail, return empty structure
                print("All fetch methods failed!")
                return {"entries": []}

            def get_proxies(proxy_list_urls):
                """Get a list of free proxies from various sources"""
                import re
                from bs4 import BeautifulSoup
                
                all_proxies = []
                ua = UserAgent()
                headers = {"User-Agent": ua.random}
                
                for url in proxy_list_urls:
                    try:
                        response = requests.get(url, headers=headers, timeout=10)
                        if response.status_code == 200:
                            soup = BeautifulSoup(response.text, 'html.parser')
                            
                            # Common pattern across many proxy list sites
                            table = soup.find('table')
                            if table:
                                for row in table.find_all('tr'):
                                    cells = row.find_all('td')
                                    if len(cells) >= 2:
                                        ip = cells[0].text.strip()
                                        port = cells[1].text.strip()
                                        if re.match(r'\d+\.\d+\.\d+\.\d+', ip) and port.isdigit():
                                            all_proxies.append(f"{ip}:{port}")
                    except Exception as e:
                        print(f"Error fetching proxies from {url}: {e}")
                
                # Randomize and limit list
                random.shuffle(all_proxies)
                return all_proxies[:10]  # Limit to 10 proxies to try
            
            def get_blog_info(feed_data, num_entries=None):
                """
                Extracts blog titles and links from feed data.
                Args:
                    feed_data: The feed data returned by fetch_substack_feed.
                    num_entries: The number of entries to extract (default: None = all entries).
                Returns:
                    A list of dictionaries, each containing the title and link for a blog entry.
                """
                entries = []
                if not feed_data or "entries" not in feed_data:
                    print("No entries found in feed data")
                    return entries
                
                # Get all entries if num_entries is None, otherwise get the specified number
                entries_to_process = feed_data["entries"] if num_entries is None else feed_data["entries"][:num_entries]
                
                for entry in entries_to_process:
                    title = entry.get("title", "Unknown Title")
                    link = entry.get("link", "#")
                    
                    entry_data = {
                        "title": title,
                        "link": link
                    }
                    entries.append(entry_data)
                
                return entries

            def update_markdown_file(filename, blog_info, start_marker, end_marker):
                """
                Updates a markdown file with blog info between specified markers.
                Args:
                    filename: The name of the markdown file.
                    blog_info: A list of dictionaries containing blog title and link.
                    start_marker: The marker indicating the start of the section to update.
                    end_marker: The marker indicating the end of the section to update.
                """
                try:
                    # Validate inputs
                    if not filename or not isinstance(filename, str):
                        print(f"Invalid filename: '{filename}'")
                        return False
                        
                    print(f"Attempting to update file: '{filename}'")
                    
                    # Make sure the directory exists
                    import os
                    dir_name = os.path.dirname(filename)
                    if dir_name:  # Only try to create directory if there is one
                        os.makedirs(dir_name, exist_ok=True)
                    
                    # Check if file exists, if not create it with initial content
                    if not os.path.exists(filename):
                        print(f"Creating new file {filename} as it doesn't exist")
                        with open(filename, 'w', encoding='utf-8') as f:
                            f.write("# Substack Blog Posts\n\n")
                            f.write(f"{start_marker}\n\n{end_marker}\n")
                    
                    # Read the existing content
                    with open(filename, 'r', encoding='utf-8') as f:
                        file_content = f.read()

                    # Find markers
                    start_index = file_content.find(start_marker)
                    if start_index == -1:
                        print(f"Start marker '{start_marker}' not found in {filename}, adding it")
                        # If start marker not found, add it to the end
                        file_content += f"\n\n{start_marker}\n\n{end_marker}\n"
                        start_index = file_content.find(start_marker)
                    
                    start_index += len(start_marker)
                    
                    end_index = file_content.find(end_marker, start_index)
                    if end_index == -1:
                        print(f"End marker '{end_marker}' not found in {filename}, adding it")
                        # If end marker not found, add it after start marker
                        new_content = file_content[:start_index] + f"\n\n{end_marker}\n"
                        file_content = new_content
                        end_index = file_content.find(end_marker, start_index)

                    # Generate the new blog entries content
                    new_content = "\n"
                    for entry in blog_info:
                        new_content += f"* [{entry['title']}]({entry['link']})\n"
                    
                    # Add a timestamp to force Git to detect a change
                    new_content += f"\n<!-- Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -->\n"

                    # Update the content between markers
                    updated_content = file_content[:start_index] + new_content + file_content[end_index:]

                    # Write back to file
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(updated_content)
                    
                    print(f"Updated {filename} successfully with {len(blog_info)} blog posts!")
                    return True
                except Exception as e:
                    print(f"Error updating markdown file: {e}")
                    import traceback
                    print(traceback.format_exc())
                    return False
            
            # Main execution
            print("Starting Substack blog post fetcher...")
            
            # Your Substack feed URL
            FEED_URL = "https://datacommons.substack.com/feed"
            
            # Fetch the feed
            feed_data = fetch_substack_feed(FEED_URL)
            
            # Print the latest 5 posts for the log
            if feed_data and "entries" in feed_data and feed_data["entries"]:
                print("\n========= LATEST 5 SUBSTACK BLOG POSTS =========\n")
                
                for i, entry in enumerate(feed_data["entries"]):
                    print(f"{i+1}. {entry.get('title', 'No title')}")
                    print(f"   URL: {entry.get('link', 'No link')}")
                    print(f"   Published: {entry.get('published', 'No date')}")
                    print("")
                
                print("===============================================")
                
                # Get blog info and update docs/substack_blogs.md
                blog_info = get_blog_info(feed_data, num_entries=None)
                
                # Define file path constants
                REPO_FILE_PATH = "docs/substack_blogs.md"
                START_MARKER = "<!-- START_MARKER -->"
                END_MARKER = "<!-- END_MARKER -->"
                
                success = update_markdown_file(REPO_FILE_PATH, blog_info, START_MARKER, END_MARKER)
                
                if success:
                    print(f"{REPO_FILE_PATH} updated successfully with the latest blog posts!")
                else:
                    print(f"Failed to update {REPO_FILE_PATH}")
            else:
                print("No blog posts found or failed to fetch the feed.")

      - name: Show fetch results
        run: cat *.log || echo "No log file found"
        
      - name: Show file after changes
        run: |
          echo "=== File contents after changes ==="
          if [ -f docs/substack_blogs.md ]; then
            cat docs/substack_blogs.md
          else
            echo "File still does not exist"
          fi
          
      - name: Commit changes with force update
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Make sure the file exists and has content
          mkdir -p docs
          touch docs/substack_blogs.md
          
          # Stage the file
          git add docs/substack_blogs.md
          
          # Check if there are any changes
          if git diff --staged --quiet; then
            echo "No changes detected in the file. Adding a timestamp to force a commit."
            # Add a timestamp to force a change
            echo "<!-- Last updated: $(date) -->" >> docs/substack_blogs.md
            git add docs/substack_blogs.md
          fi
          
          # Commit and push to master branch explicitly
          git commit -m "Updated Substack blogs"
          git push origin master
